name: Build

on:
  push:
    branches:
      - main

permissions:
  contents: write

jobs:
  macos:
    runs-on: macos-latest

    strategy:
      matrix:
        include:
          - build: 'metal'
            defines: '-DLLAMA_NATIVE=OFF -DLLAMA_FAST=ON -DLLAMA_BUILD_SERVER=ON -DBUILD_SHARED_LIBS=ON'
          - build: 'nometal'
            defines: '-DLLAMA_NATIVE=OFF -DLLAMA_FAST=ON -DLLAMA_BUILD_SERVER=ON -DBUILD_SHARED_LIBS=ON -DLLAMA_NO_METAL=1'

    steps:
      - name: Checkout code
        uses: actions/checkout@v2
        with:
          repository: ggerganov/llama.cpp

      - name: Compile with cmake.
        run: |
          mkdir build
          cd build
          cmake .. ${{ matrix.defines }}
          cmake --build . --config Release

      - name: Pack Artifacts
        run: |
          cd build/bin
          ls

  # windows-cpu-only:
  #   runs-on: windows-latest

  #   strategy:
  #     matrix:
  #       include:
  #         - build: 'noavx'
  #           defines: '-DLLAMA_NATIVE=OFF -DLLAMA_FAST=ON -DLLAMA_BUILD_SERVER=ON -DLLAMA_AVX=OFF -DLLAMA_AVX2=OFF -DLLAMA_FMA=OFF -DBUILD_SHARED_LIBS=ON'
  #         - build: 'avx2'
  #           defines: '-DLLAMA_NATIVE=OFF -DLLAMA_FAST=ON -DLLAMA_BUILD_SERVER=ON -DBUILD_SHARED_LIBS=ON'
  #         - build: 'avx'
  #           defines: '-DLLAMA_NATIVE=OFF -DLLAMA_FAST=ON -DLLAMA_BUILD_SERVER=ON -DLLAMA_AVX2=OFF -DBUILD_SHARED_LIBS=ON'
  #         - build: 'avx512'
  #           defines: '-DLLAMA_NATIVE=OFF -DLLAMA_FAST=ON -DLLAMA_BUILD_SERVER=ON -DLLAMA_AVX512=ON -DBUILD_SHARED_LIBS=ON'
  
  #   steps:
  #     - name: Checkout code
  #       uses: actions/checkout@v2
  #       with:
  #         repository: ggerganov/llama.cpp

  #     - name: Compile with cmake.
  #       run: |
  #         mkdir build
  #         cd build
  #         cmake .. ${{ matrix.defines }}
  #         cmake --build . --config Release

  #     - name: Pack Artifacts
  #       run: |
  #         cd build/bin
  #         ls Release

  # windows-cuda:
  #   runs-on: windows-latest

  #   strategy:
  #     matrix:
  #       include:
  #         - build: 'cublas'
  #           defines: '-DLLAMA_NATIVE=OFF -DLLAMA_FAST=ON -DLLAMA_BUILD_SERVER=ON -DLLAMA_CUBLAS=ON -DBUILD_SHARED_LIBS=ON'

  #   steps:
  #     - name: Checkout code
  #       uses: actions/checkout@v2
  #       with:
  #         repository: ggerganov/llama.cpp

  #     - name: Install CUDA
  #       uses: Jimver/cuda-toolkit@v0.2.11
  #       id: cuda-toolkit
  #       with:
  #         cuda: '12.2.0'
  #         method: 'network'
  #         sub-packages: '["nvcc", "cudart", "cublas", "cublas_dev", "thrust", "visual_studio_integration"]'

  #     - name: Compile with cmake.
  #       run: |
  #         mkdir build
  #         cd build
  #         cmake .. ${{ matrix.defines }}
  #         cmake --build . --config Release

  #     - name: Pack Artifacts
  #       run: |
  #         cd build/bin
  #         ls Release